{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirogami/2020sCmpEngQnn/blob/main/Plato__Fine_Tune_DistilGPT2_and_Generate_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ziTUQXO7Yxve"
      },
      "outputs": [],
      "source": [
        "# refert to https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Ye9GZEhpZRtz",
        "outputId": "b7033856-aa14-4630-d15e-9bb99ad5100d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0                                   sentence_lowered\n",
              "0               0   what's new, socrates, to make you leave your ...\n",
              "1               1  surely you are not prosecuting anyone before t...\n",
              "2               2  the athenians do not call this a prosecution b...\n",
              "3               3                              what is this you say?\n",
              "4               4  someone must have indicted you, for you are no...\n",
              "...           ...                                                ...\n",
              "38360       38361  now i despise life, since i'm moving to a bett...\n",
              "38361       38362  and now i'd like to go over what you've said, ...\n",
              "38362       38363      but after midday, socrates, please visit me.'\n",
              "38363       38364                            i will do what you ask.\n",
              "38364       38365  and now i'll go back to my walk to the cynosar...\n",
              "\n",
              "[38365 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce2bff64-360b-46c7-9be2-49de2f68fc67\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentence_lowered</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>what's new, socrates, to make you leave your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>surely you are not prosecuting anyone before t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>the athenians do not call this a prosecution b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>what is this you say?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>someone must have indicted you, for you are no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38360</th>\n",
              "      <td>38361</td>\n",
              "      <td>now i despise life, since i'm moving to a bett...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38361</th>\n",
              "      <td>38362</td>\n",
              "      <td>and now i'd like to go over what you've said, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38362</th>\n",
              "      <td>38363</td>\n",
              "      <td>but after midday, socrates, please visit me.'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38363</th>\n",
              "      <td>38364</td>\n",
              "      <td>i will do what you ask.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38364</th>\n",
              "      <td>38365</td>\n",
              "      <td>and now i'll go back to my walk to the cynosar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38365 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce2bff64-360b-46c7-9be2-49de2f68fc67')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce2bff64-360b-46c7-9be2-49de2f68fc67 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce2bff64-360b-46c7-9be2-49de2f68fc67');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data = pd.read_csv('plato.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gAbMLfxrZbKG"
      },
      "outputs": [],
      "source": [
        "texts = list(set(data['sentence_lowered']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LotaG9qgZmHy"
      },
      "outputs": [],
      "source": [
        "file_name = 'testing.txt'\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(\" |EndOfText|\\n\".join(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwFvrFRjOwo"
      },
      "source": [
        "Now, let's come to Transformers by Huggingface, and unleash the Transformers (Autobots... just kidding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkocIBHfaZul",
        "outputId": "ae24db82-c3ac-4444-e1aa-12b48e666224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-d899k1kv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-d899k1kv\n",
            "  Resolved https://github.com/huggingface/transformers to commit b7bb2b59f72504fbabe3de24c84b5e282c4870e8\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.27.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0.dev0) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.27.0.dev0) (4.0.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.27.0.dev0-py3-none-any.whl size=6435070 sha256=6eb62ff32743baf3d8b2f57ca65bc5a481883bd560dc47987475aa228cee2f9e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-br2suudt/wheels/42/68/45/c63edff61c292f2dfd4df4ef6522dcbecc603e7af82813c1d7\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.27.0.dev0\n"
          ]
        }
      ],
      "source": [
        "# install the latest version of transformers with pip\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv-M2dquKT8j",
        "outputId": "67a5d2a8-64ad-4c1f-d182-ade6e077d144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 126752, done.\u001b[K\n",
            "remote: Counting objects: 100% (2228/2228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1139/1139), done.\u001b[K\n",
            "remote: Total 126752 (delta 1161), reused 1898 (delta 1070), pack-reused 124524\u001b[K\n",
            "Receiving objects: 100% (126752/126752), 125.24 MiB | 17.53 MiB/s, done.\n",
            "Resolving deltas: 100% (94660/94660), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIJfTnDjmG6"
      },
      "source": [
        "Make 2 directories. \n",
        "\n",
        "1) weights - for storing the weights of distilgpt2\n",
        "\n",
        "2) tokenizer - for storing the tokenizer of distilgpt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lxyJj4JhCSQa"
      },
      "outputs": [],
      "source": [
        "# create the directories with mkdir in the terminal\n",
        "!mkdir weights tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhH0xD1-qnh-",
        "outputId": "1e51ac8c-43c0-4604-f607-81344219437d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HiZgs-kFLg"
      },
      "source": [
        "Now, its time for Training (or fine tuning) distilgpt2 with IMDB reviews\n",
        "Given below is a command containing few parameters to help Transformers finetune distilgpt2. now, let's understand what these parameters mean\n",
        "\n",
        "1) output_dir: It is the weights_dir we made where our finetuned model will be stored in the form of checkpoints\n",
        "\n",
        "2) model_name_or_path: It tells the kind of model we are currently dealing with\n",
        "\n",
        "3) per_device_train_batch_size: It tells the batch size for each gpu\n",
        "\n",
        "4) do_train: It tells pytorch to start training mode\n",
        "\n",
        "5) train_file: This is where we give the input text data \n",
        "\n",
        "6) num_train_epochs: Number of epochs for finetuning\n",
        "\n",
        "\n",
        "Now, let the training begin..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "403xwtooZWU-",
        "outputId": "80e12a0c-c096-4e8b-cf4d-c5556263f29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.12.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.25.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2023.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J_rFZRe_2KAW"
      },
      "outputs": [],
      "source": [
        "weights_dir = \"output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "42gszj3gkUU1"
      },
      "outputs": [],
      "source": [
        "cmd = '''\n",
        "python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file {0} \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --output_dir {1}\n",
        "'''.format(file_name, weights_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQKT9jlOcnjY",
        "outputId": "107f71b9-d7d0-46e3-8aeb-9da94c975d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Feb06_22-55-00_520a4c4b3a10,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=output,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-76b539a4e2a0a55f\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Generating dataset text (/root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 4987.28it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 999.12it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 160.01it/s]\n",
            "WARNING:datasets.builder:Using custom data configuration default-76b539a4e2a0a55f\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Using custom data configuration default-76b539a4e2a0a55f\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/text\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2\n",
            "Downloading (…)lve/main/config.json: 100% 762/762 [00:00<00:00, 30.0kB/s]\n",
            "[INFO|configuration_utils.py:664] 2023-02-06 22:55:06,141 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/config.json\n",
            "[INFO|configuration_utils.py:716] 2023-02-06 22:55:06,142 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:460] 2023-02-06 22:55:07,083 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:664] 2023-02-06 22:55:08,013 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/config.json\n",
            "[INFO|configuration_utils.py:716] 2023-02-06 22:55:08,015 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:01<00:00, 787kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 413kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.02MB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-06 22:55:20,101 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:664] 2023-02-06 22:55:20,102 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/config.json\n",
            "[INFO|configuration_utils.py:716] 2023-02-06 22:55:20,103 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.27.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 353M/353M [00:03<00:00, 99.1MB/s]\n",
            "[INFO|modeling_utils.py:2327] 2023-02-06 22:55:24,667 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:572] 2023-02-06 22:55:24,946 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2914] 2023-02-06 22:55:26,478 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2922] 2023-02-06 22:55:26,478 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Downloading (…)neration_config.json: 100% 124/124 [00:00<00:00, 18.9kB/s]\n",
            "[INFO|configuration_utils.py:534] 2023-02-06 22:55:28,308 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/generation_config.json\n",
            "[INFO|configuration_utils.py:572] 2023-02-06 22:55:28,308 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"transformers_version\": \"4.27.0.dev0\"\n",
            "}\n",
            "\n",
            "Running tokenizer on dataset:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-d9e68f55f389427a.arrow\n",
            "Running tokenizer on dataset: 100% 37/37 [00:02<00:00, 13.27ba/s]\n",
            "Running tokenizer on dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1c44b97a8e8953c7.arrow\n",
            "Running tokenizer on dataset: 100% 2/2 [00:00<00:00, 14.26ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/37 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-c7fc1462c5ae0e33.arrow\n",
            "Grouping texts in chunks of 1024: 100% 37/37 [00:01<00:00, 26.36ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/text/default-76b539a4e2a0a55f/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-1deed29457c7ae34.arrow\n",
            "Grouping texts in chunks of 1024: 100% 2/2 [00:00<00:00, 26.56ba/s]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1677] 2023-02-06 22:55:39,136 >> ***** Running training *****\n",
            "[INFO|trainer.py:1678] 2023-02-06 22:55:39,137 >>   Num examples = 1114\n",
            "[INFO|trainer.py:1679] 2023-02-06 22:55:39,137 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1680] 2023-02-06 22:55:39,137 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1681] 2023-02-06 22:55:39,137 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1682] 2023-02-06 22:55:39,137 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1683] 2023-02-06 22:55:39,137 >>   Total optimization steps = 1671\n",
            "[INFO|trainer.py:1684] 2023-02-06 22:55:39,137 >>   Number of trainable parameters = 81912576\n",
            "{'loss': 3.2197, 'learning_rate': 3.5038898862956315e-05, 'epoch': 0.9}\n",
            " 30% 500/1671 [03:40<08:30,  2.29it/s][INFO|trainer.py:2753] 2023-02-06 22:59:19,299 >> Saving model checkpoint to output/checkpoint-500\n",
            "[INFO|configuration_utils.py:453] 2023-02-06 22:59:19,300 >> Configuration saved in output/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:359] 2023-02-06 22:59:19,301 >> Configuration saved in output/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1720] 2023-02-06 22:59:20,289 >> Model weights saved in output/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-06 22:59:20,289 >> tokenizer config file saved in output/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-06 22:59:20,290 >> Special tokens file saved in output/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 3.0608, 'learning_rate': 2.0077797725912628e-05, 'epoch': 1.8}\n",
            " 60% 1000/1671 [07:22<04:53,  2.29it/s][INFO|trainer.py:2753] 2023-02-06 23:03:01,449 >> Saving model checkpoint to output/checkpoint-1000\n",
            "[INFO|configuration_utils.py:453] 2023-02-06 23:03:01,450 >> Configuration saved in output/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:359] 2023-02-06 23:03:01,451 >> Configuration saved in output/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:1720] 2023-02-06 23:03:02,565 >> Model weights saved in output/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-06 23:03:02,566 >> tokenizer config file saved in output/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-06 23:03:02,566 >> Special tokens file saved in output/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 3.0072, 'learning_rate': 5.116696588868941e-06, 'epoch': 2.69}\n",
            " 90% 1500/1671 [11:04<01:14,  2.30it/s][INFO|trainer.py:2753] 2023-02-06 23:06:43,808 >> Saving model checkpoint to output/checkpoint-1500\n",
            "[INFO|configuration_utils.py:453] 2023-02-06 23:06:43,809 >> Configuration saved in output/checkpoint-1500/config.json\n",
            "[INFO|configuration_utils.py:359] 2023-02-06 23:06:43,810 >> Configuration saved in output/checkpoint-1500/generation_config.json\n",
            "[INFO|modeling_utils.py:1720] 2023-02-06 23:06:44,707 >> Model weights saved in output/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-06 23:06:44,707 >> tokenizer config file saved in output/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-06 23:06:44,708 >> Special tokens file saved in output/checkpoint-1500/special_tokens_map.json\n",
            "100% 1671/1671 [12:22<00:00,  2.30it/s][INFO|trainer.py:1945] 2023-02-06 23:08:02,064 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 742.9493, 'train_samples_per_second': 4.498, 'train_steps_per_second': 2.249, 'train_loss': 3.0846177562848314, 'epoch': 3.0}\n",
            "100% 1671/1671 [12:22<00:00,  2.25it/s]\n",
            "[INFO|trainer.py:2753] 2023-02-06 23:08:02,092 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:453] 2023-02-06 23:08:02,093 >> Configuration saved in output/config.json\n",
            "[INFO|configuration_utils.py:359] 2023-02-06 23:08:02,094 >> Configuration saved in output/generation_config.json\n",
            "[INFO|modeling_utils.py:1720] 2023-02-06 23:08:02,934 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-06 23:08:02,934 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-06 23:08:02,934 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     3.0846\n",
            "  train_runtime            = 0:12:22.94\n",
            "  train_samples            =       1114\n",
            "  train_samples_per_second =      4.498\n",
            "  train_steps_per_second   =      2.249\n",
            "[INFO|modelcard.py:449] 2023-02-06 23:08:03,974 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl2zr2Gm2Db"
      },
      "source": [
        "Although, Huggingface provides a run_generation.py file for language generation. Running it from a command (as it takes the input), makes it load the model and the tokenizer everytime you run the file which slows downs generation. To reduce the I/O overhead, I have restructured the run_generation.py file in the following code which only loads the model and tokenizer once in a model and a tokenizer object and we can use these objects to generate text over and over again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M7_-L9gGziiv"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def get_model_tokenizer(weights_dir, device = 'cuda'):\n",
        "    print(\"Loading Model ...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(weights_dir)\n",
        "    model.to('cuda')\n",
        "    print(\"Model Loaded ...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = 0.7,\n",
        "    k=20,\n",
        "    p=0.9,\n",
        "    repetition_penalty = 1.0,\n",
        "    device = 'cuda'\n",
        "):\n",
        "\n",
        "    MAX_LENGTH = int(10000)\n",
        "    def adjust_length_to_model(length, max_sequence_length):\n",
        "        if length < 0 and max_sequence_length > 0:\n",
        "            length = max_sequence_length\n",
        "        elif 0 < max_sequence_length < length:\n",
        "            length = max_sequence_length  # No generation bigger than model size\n",
        "        elif length < 0:\n",
        "            length = MAX_LENGTH  # avoid infinite loop\n",
        "        return length\n",
        "        \n",
        "    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return generated_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q3fwyPATznLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de73dfa-9b22-4971-f7a1-154dbd4eeb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model ...\n",
            "Model Loaded ...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model, tokenizer = get_model_tokenizer(weights_dir, device = 'cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EEZ94LsNsy9s"
      },
      "outputs": [],
      "source": [
        "temperature = 1.0\n",
        "k=400\n",
        "p=0.9\n",
        "repetition_penalty = 1.0\n",
        "num_return_sequences = 5\n",
        "length = 1000\n",
        "stop_token = '|EndOfText|'\n",
        "prompt_text = \"The truth is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xfiEt4jEszEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c848512-71e3-427a-e270-340f3ce786c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.2 s, sys: 939 ms, total: 17.2 s\n",
            "Wall time: 22.8 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The truth is that, despite being a man, he is wise. ',\n",
              " 'The truth is that all these have many properties, and many of them are relatively simple and never will be readily apparent to the eyes of those in possession of them. ',\n",
              " \"The truth is, so far as my money goes, we don't care to take care of our present state, the other candidates we select. \",\n",
              " 'The truth is: men are the best, with the exception of the animals. ',\n",
              " 'The truth is that each of these studies is valuable for the sake of others, and i do not know how or when i should trust anyone who will give up my argument about it, either on your behalf or on the judgment of a political leader. ']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "%%time\n",
        "generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = temperature,\n",
        "    k=k,\n",
        "    p=p,\n",
        "    repetition_penalty = repetition_penalty\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "97LApghm7gaK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}